import os
import numpy as np
import pandas as pd
import joblib
import json
from typing import Dict, List, Tuple, Optional, Union
import wfdb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import time


class PhysioNetBPPredictor:
    """
    PhysioNet ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í˜ˆì•• ì˜ˆì¸¡ ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    - íŠ¹ì„± ì¶”ì¶œ ë° ì¤€ë¹„
    - ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
    - íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    - ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ
    """
    
    def __init__(self, data_dir: str = 'data'):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_dir: PhysioNet ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.data_dir = data_dir
        self.models = {}
        self.scaler = None
        self.feature_names = []
        self.metadata = {}
        
        print("ğŸ©º PhysioNetBPPredictor ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ================================================================
    # ë°ì´í„° ë¡œë“œ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def extract_patient_info(self, record, record_name: str) -> Optional[Dict]:
        """
        WFDB ë ˆì½”ë“œì—ì„œ í™˜ì ì •ë³´ ì¶”ì¶œ
        
        Args:
            record: WFDB record ê°ì²´
            record_name: ë ˆì½”ë“œ ì´ë¦„
        
        Returns:
            í™˜ì ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            patient_data = {
                'patient_id': record_name,
                'sampling_rate': record.fs,
                'signal_length': record.sig_len
            }
            
            if record.p_signal is not None and len(record.p_signal) > 0:
                for i, sig_name in enumerate(record.sig_name):
                    signal_data = record.p_signal[:, i]
                    patient_data[f'{sig_name}_mean'] = np.mean(signal_data)
                    patient_data[f'{sig_name}_std'] = np.std(signal_data)
                    patient_data[f'{sig_name}_max'] = np.max(signal_data)
                    patient_data[f'{sig_name}_min'] = np.min(signal_data)
            
            if hasattr(record, 'comments') and record.comments:
                for comment in record.comments:
                    if ':' in comment:
                        key, value = comment.split(':', 1)
                        patient_data[key.strip()] = value.strip()
            
            return patient_data
            
        except Exception as e:
            print(f"âš ï¸ í™˜ì ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({record_name}): {str(e)}")
            return None
    
    def load_all_patient_data(self, max_records: Optional[int] = None, 
                            extract_features: bool = True) -> pd.DataFrame:
        """
        ëª¨ë“  í™˜ì ë°ì´í„°ë¥¼ ë¡œë“œ
        
        Args:
            max_records: ì½ì„ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            extract_features: Trueë©´ ì‹ í˜¸ íŠ¹ì„± ì¶”ì¶œ, Falseë©´ ë©”íƒ€ë°ì´í„°ë§Œ
        
        Returns:
            ëª¨ë“  í™˜ì ë°ì´í„°ê°€ í¬í•¨ëœ DataFrame
        """
        print(f"ğŸš€ ëª¨ë“  í™˜ì ë°ì´í„° ë¡œë“œ ì‹œì‘: {self.data_dir}")
        
        hea_files = sorted([f for f in os.listdir(self.data_dir) if f.endswith('.hea')])
        
        if not hea_files:
            raise FileNotFoundError(f"âŒ {self.data_dir}ì— .hea íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        if max_records:
            hea_files = hea_files[:max_records]
        
        print(f"ğŸ“Š ì²˜ë¦¬í•  í™˜ì ë°ì´í„°: {len(hea_files)}ê°œ")
        
        all_patient_data = []
        success_count = 0
        fail_count = 0
        
        for idx, hea_file in enumerate(hea_files, 1):
            try:
                record_name = hea_file.replace('.hea', '')
                record_path = os.path.join(self.data_dir, record_name)
                record = wfdb.rdrecord(record_path)
                
                if extract_features:
                    patient_data = self.extract_patient_info(record, record_name)
                    if patient_data:
                        all_patient_data.append(patient_data)
                        success_count += 1
                else:
                    patient_data = {
                        'record_name': record_name,
                        'sampling_rate': record.fs,
                        'signal_length': record.sig_len,
                        'n_signals': record.n_sig,
                        'signal_names': ', '.join(record.sig_name),
                        'units': ', '.join(record.units)
                    }
                    all_patient_data.append(patient_data)
                    success_count += 1
                
                if idx % 100 == 0:
                    print(f"  â³ ì§„í–‰ ì¤‘... {idx}/{len(hea_files)} ({idx/len(hea_files)*100:.1f}%)")
                    
            except Exception as e:
                fail_count += 1
                if fail_count <= 5:
                    print(f"  âš ï¸ {hea_file} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                continue
        
        df = pd.DataFrame(all_patient_data)
        
        print(f"\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        print(f"   - ì„±ê³µ: {success_count}ê°œ")
        print(f"   - ì‹¤íŒ¨: {fail_count}ê°œ")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
        print(f"   - ë°ì´í„° shape: {df.shape}")
        
        return df
    
    # ================================================================
    # ì „ì²˜ë¦¬ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def preprocess_data(self, df: pd.DataFrame, target_column: str = 'NIBP_mean',
                       remove_target_outliers: bool = True) -> pd.DataFrame:
        """
        PhysioNet ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            df: ì›ë³¸ DataFrame
            target_column: ì˜ˆì¸¡ ëŒ€ìƒ ì»¬ëŸ¼
            remove_target_outliers: íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ì´ìƒì¹˜ ì œê±° ì—¬ë¶€
        
        Returns:
            ì „ì²˜ë¦¬ëœ DataFrame
        """
        print("=" * 60)
        print("ğŸ”„ PhysioNet ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)
        
        df = df.copy()
        print(f"\nğŸ“Š ì´ˆê¸° ë°ì´í„° shape: {df.shape}")
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        print("\n2ï¸âƒ£ ê²°ì¸¡ê°’ ì²˜ë¦¬")
        missing_before = df.isnull().sum().sum()
        print(f"   - ì²˜ë¦¬ ì „ ê²°ì¸¡ê°’: {missing_before}ê°œ")
        
        if missing_before > 0:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
            
            categorical_cols = df.select_dtypes(include=['object']).columns
            for col in categorical_cols:
                if col != 'patient_id':
                    mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'
                    df[col] = df[col].fillna(mode_val)
            
            print(f"   - ì²˜ë¦¬ í›„ ê²°ì¸¡ê°’: {df.isnull().sum().sum()}ê°œ")
        
        # ë¬´í•œëŒ€ ê°’ ë° ì´ìƒì¹˜ ì²˜ë¦¬
        print("\n3ï¸âƒ£ ë¬´í•œëŒ€ ê°’ ë° ì´ìƒì¹˜ ì²˜ë¦¬")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if col in ['patient_id', 'sampling_rate', 'signal_length']:
                continue
            
            inf_count = np.isinf(df[col]).sum()
            if inf_count > 0:
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                df[col] = df[col].fillna(df[col].median())
            
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            if IQR > 0:
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì´ìƒì¹˜ ì œê±°
        if remove_target_outliers and target_column in df.columns:
            print(f"\n4ï¸âƒ£ íƒ€ê²Ÿ ë³€ìˆ˜ ({target_column}) ì´ìƒì¹˜ ì œê±°")
            before_count = len(df)
            
            if 'NIBP' in target_column:
                valid_range = (40, 200)
                df = df[(df[target_column] >= valid_range[0]) & 
                       (df[target_column] <= valid_range[1])]
                removed = before_count - len(df)
                if removed > 0:
                    print(f"   - ë¹„ì •ìƒ ë²”ìœ„ ì œê±°: {removed}ê°œ")
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        print("\n5ï¸âƒ£ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
        
        if 'ECG1_mean' in df.columns and 'ECG2_mean' in df.columns:
            df['ECG_diff_mean'] = abs(df['ECG1_mean'] - df['ECG2_mean'])
            df['ECG_avg_mean'] = (df['ECG1_mean'] + df['ECG2_mean']) / 2
            print("   - ECG ì‹ í˜¸ ì°¨ì´ ë° í‰ê·  ê³„ì‚° âœ…")
        
        if 'NIBP_std' in df.columns and 'NIBP_mean' in df.columns:
            df['NIBP_cv'] = df['NIBP_std'] / (df['NIBP_mean'] + 1e-8)
            print("   - NIBP ë³€ë™ê³„ìˆ˜ ê³„ì‚° âœ…")
        
        if 'signal_length' in df.columns and 'sampling_rate' in df.columns:
            df['duration_minutes'] = df['signal_length'] / df['sampling_rate'] / 60
            print("   - ì‹ í˜¸ ê¸¸ì´(ë¶„) ê³„ì‚° âœ…")
        
        print("\n" + "=" * 60)
        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
        print("=" * 60)
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„° shape: {df.shape}")
        
        return df
    
    # ================================================================
    # íŠ¹ì„± ì¤€ë¹„ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def prepare_features(self, df: pd.DataFrame, target_col: str = 'NIBP_mean',
                        exclude_nibp_features: bool = True) -> Tuple[pd.DataFrame, pd.Series, List[str]]:
        """
        ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜(y) ë¶„ë¦¬
        
        Args:
            df: ì „ì²˜ë¦¬ëœ DataFrame
            target_col: íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ëª…
            exclude_nibp_features: NIBP ê´€ë ¨ íŠ¹ì„± ì œì™¸ ì—¬ë¶€
        
        Returns:
            X (DataFrame): ë…ë¦½ ë³€ìˆ˜
            y (Series): ì¢…ì† ë³€ìˆ˜
            feature_names (list): íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        print("=" * 60)
        print("ğŸ¯ íŠ¹ì„± ì¤€ë¹„ ì‹œì‘")
        print("=" * 60)
        
        df = df.copy()
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
        if target_col not in df.columns:
            nibp_candidates = [col for col in df.columns if 'NIBP' in col]
            if 'NIBP_max' in nibp_candidates:
                target_col = 'NIBP_max'
            elif 'NIBP_mean' in nibp_candidates:
                target_col = 'NIBP_mean'
            else:
                raise ValueError(f"âŒ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        y = df[target_col]
        print(f"\níƒ€ê²Ÿ ë³€ìˆ˜: {target_col}")
        print(f"   - í‰ê· : {y.mean():.2f}")
        print(f"   - í‘œì¤€í¸ì°¨: {y.std():.2f}")
        print(f"   - ë²”ìœ„: {y.min():.2f} ~ {y.max():.2f}")
        
        # ì œì™¸í•  ì»¬ëŸ¼ ì„ íƒ
        exclude_cols = []
        
        id_cols = [col for col in df.columns if any(x in col.lower() 
                  for x in ['id', 'patient', 'record_name'])]
        exclude_cols.extend(id_cols)
        exclude_cols.append(target_col)
        
        if exclude_nibp_features and 'NIBP' in target_col:
            nibp_cols = [col for col in df.columns if 'NIBP' in col and col != target_col]
            exclude_cols.extend(nibp_cols)
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        X = df[feature_cols]
        
        print(f"\níŠ¹ì„± ê°œìˆ˜: {len(feature_cols)}ê°œ")
        print(f"ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ")
        
        self.feature_names = feature_cols
        
        return X, y, feature_cols
    
    # ================================================================
    # ëª¨ë¸ í•™ìŠµ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def train_models(self, X: pd.DataFrame, y: pd.Series, 
                    test_size: float = 0.2, random_state: int = 42,
                    models_to_train: Optional[List[str]] = None) -> Dict:
        """
        í˜ˆì•• ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            random_state: ëœë¤ ì‹œë“œ
            models_to_train: í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 70)
        print("ğŸ¤– ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 70)
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        print(f"\ní›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ëª¨ë¸ ì •ì˜
        all_models = {
            'Ridge': {
                'model': Ridge(alpha=1.0, random_state=random_state),
                'scaled': True,
                'description': 'ë¦¿ì§€ íšŒê·€ (L2 ì •ê·œí™”)'
            },
            'RandomForest': {
                'model': RandomForestRegressor(
                    n_estimators=100, max_depth=15,
                    min_samples_split=5, min_samples_leaf=2,
                    random_state=random_state, n_jobs=-1
                ),
                'scaled': False,
                'description': 'ëœë¤ í¬ë ˆìŠ¤íŠ¸'
            }
        }
        
        if models_to_train is None:
            models_to_train = ['Ridge', 'RandomForest']
        
        selected_models = {name: all_models[name] for name in models_to_train 
                          if name in all_models}
        
        results = {}
        
        for name, model_info in selected_models.items():
            print(f"\n{'='*60}")
            print(f"ğŸ”„ {name} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            print(f"{'='*60}")
            
            model = model_info['model']
            use_scaled = model_info['scaled']
            
            X_tr = X_train_scaled if use_scaled else X_train
            X_te = X_test_scaled if use_scaled else X_test
            
            start_time = time.time()
            model.fit(X_tr, y_train)
            train_time = time.time() - start_time
            
            train_pred = model.predict(X_tr)
            test_pred = model.predict(X_te)
            
            train_mae = mean_absolute_error(y_train, train_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
            train_r2 = r2_score(y_train, train_pred)
            
            test_mae = mean_absolute_error(y_test, test_pred)
            test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
            test_r2 = r2_score(y_test, test_pred)
            
            results[name] = {
                'model': model,
                'scaled': use_scaled,
                'train_mae': train_mae,
                'train_rmse': train_rmse,
                'train_r2': train_r2,
                'test_mae': test_mae,
                'test_rmse': test_rmse,
                'test_r2': test_r2,
                'train_predictions': train_pred,
                'test_predictions': test_pred,
                'train_time': train_time
            }
            
            print(f"\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   í…ŒìŠ¤íŠ¸ MAE: {test_mae:.2f} mmHg")
            print(f"   í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.2f} mmHg")
            print(f"   í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.3f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = min(results.keys(), key=lambda x: results[x]['test_mae'])
        results['best_model_name'] = best_model_name
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        
        results.update({
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'y_test': y_test,
            'scaler': self.scaler,
            'X_train_scaled': X_train_scaled,
            'X_test_scaled': X_test_scaled
        })
        
        self.models = results
        
        return results
    
    # ================================================================
    # ì˜ˆì¸¡ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def predict(self, X_new: Union[pd.DataFrame, Dict], 
               model_type: str = 'RandomForest') -> Dict:
        """
        ìƒˆë¡œìš´ ë°ì´í„°ì˜ í˜ˆì•• ì˜ˆì¸¡
        
        Args:
            X_new: ìƒˆë¡œìš´ í™˜ì ë°ì´í„° (dict ë˜ëŠ” DataFrame)
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (systolic_bp, diastolic_bp í¬í•¨)
        """
        # ì…ë ¥ ë°ì´í„° ë³€í™˜
        if isinstance(X_new, dict):
            X_new = pd.DataFrame([X_new])
        elif isinstance(X_new, pd.Series):
            X_new = pd.DataFrame([X_new])
        else:
            X_new = X_new.copy()
        
        # ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        if not self.models or model_type not in self.models:
            return self._basic_prediction(X_new)
        
        # íŠ¹ì„± ê²€ì¦ ë° ì„ íƒ
        try:
            X_pred_df = X_new[self.feature_names]
        except KeyError:
            # í•„ìš”í•œ íŠ¹ì„±ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì˜ˆì¸¡ ì‚¬ìš©
            return self._basic_prediction(X_new)
        
        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model = self.models[model_type]['model']
        use_scaled = self.models[model_type]['scaled']
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        if use_scaled and self.scaler is not None:
            X_pred = self.scaler.transform(X_pred_df)
        else:
            X_pred = X_pred_df
        
        # ì˜ˆì¸¡
        predictions = model.predict(X_pred)
        
        # ê²°ê³¼ ë°˜í™˜ (Streamlit appê³¼ í˜¸í™˜ë˜ëŠ” í˜•ì‹)
        # predictionsëŠ” NIBP_mean ë˜ëŠ” ë‹¤ë¥¸ í˜ˆì•• ê°’ì„ ì˜ˆì¸¡
        # ìˆ˜ì¶•ê¸°/ì´ì™„ê¸°ë¡œ ë³€í™˜
        systolic = predictions[0] if len(predictions) > 0 else 120
        diastolic = systolic * 0.67  # ëŒ€ëµì ì¸ ë¹„ìœ¨
        
        # RandomForestì¸ ê²½ìš° ì‹ ë¢° êµ¬ê°„ ê³„ì‚°
        confidence_interval = None
        if model_type == 'RandomForest' and hasattr(model, 'estimators_'):
            tree_predictions = np.array([tree.predict(X_pred) 
                                        for tree in model.estimators_])
            pred_std = np.std(tree_predictions, axis=0)
            
            confidence_interval = {
                'lower': predictions - 1.96 * pred_std,
                'upper': predictions + 1.96 * pred_std,
                'std': pred_std
            }
        
        results = {
            'systolic_bp': float(systolic),
            'diastolic_bp': float(diastolic),
            'predicted_bp': predictions,
            'model_used': model_type,
            'n_samples': len(predictions)
        }
        
        if confidence_interval is not None:
            results['confidence_interval'] = confidence_interval
        
        return results
    
    def _basic_prediction(self, patient_data: Union[pd.DataFrame, Dict]) -> Dict:
        """
        ëª¨ë¸ì´ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜
        
        Args:
            patient_data: í™˜ì ë°ì´í„°
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # DataFrameì„ dictë¡œ ë³€í™˜
        if isinstance(patient_data, pd.DataFrame):
            if len(patient_data) > 0:
                patient_dict = patient_data.iloc[0].to_dict()
            else:
                patient_dict = {}
        else:
            patient_dict = patient_data
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        age = patient_dict.get('age', 45)
        bmi = patient_dict.get('bmi', 23.0)
        smoking = patient_dict.get('smoking', 0)
        exercise_frequency = patient_dict.get('exercise_frequency', 2)
        stress_level = patient_dict.get('stress_level', 5)
        
        # ê¸°ë³¸ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜
        base_systolic = 100 + (age * 0.5)
        base_diastolic = 60 + (age * 0.3)
        
        # ìœ„í—˜ ìš”ì¸ ë°˜ì˜
        if bmi >= 30:
            base_systolic += 10
            base_diastolic += 5
        elif bmi >= 25:
            base_systolic += 5
            base_diastolic += 3
        
        if smoking == 1:
            base_systolic += 5
            base_diastolic += 3
        
        if exercise_frequency < 2:
            base_systolic += 3
            base_diastolic += 2
        
        if stress_level >= 7:
            base_systolic += 5
            base_diastolic += 3
        
        return {
            'systolic_bp': float(base_systolic),
            'diastolic_bp': float(base_diastolic),
            'model_used': 'ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜',
            'n_samples': 1
        }
    
    # ================================================================
    # ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================
    
    def get_feature_importance(self, model_type: str = 'RandomForest',
                              top_n: int = 20) -> pd.DataFrame:
        """
        ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì…
            top_n: í‘œì‹œí•  ìƒìœ„ íŠ¹ì„± ê°œìˆ˜
        
        Returns:
            íŠ¹ì„± ì¤‘ìš”ë„ DataFrame
        """
        if model_type not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_type}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        model = self.models[model_type]['model']
        
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_)
        else:
            raise ValueError(f"{model_type} ëª¨ë¸ì€ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        return importance_df
    
    # ================================================================
    # Streamlit ì§€ì› ë©”ì„œë“œ
    # ================================================================
    
    def get_model_summary(self) -> Dict:
        """
        í•™ìŠµëœ ëª¨ë¸ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜ (Streamlit í‘œì‹œìš©)
        
        Returns:
            ëª¨ë¸ ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.models:
            return {'status': 'no_models', 'message': 'í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.'}
        
        best_name = self.models.get('best_model_name')
        if not best_name:
            return {'status': 'no_best_model', 'message': 'ìµœê³  ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
        
        best_result = self.models[best_name]
        
        summary = {
            'status': 'success',
            'best_model': best_name,
            'test_mae': best_result['test_mae'],
            'test_rmse': best_result['test_rmse'],
            'test_r2': best_result['test_r2'],
            'train_time': best_result.get('train_time', 0),
            'n_features': len(self.feature_names),
            'available_models': [k for k in self.models.keys() 
                               if k not in ['best_model_name', 'X_train', 'X_test', 
                                          'y_train', 'y_test', 'scaler', 
                                          'X_train_scaled', 'X_test_scaled']]
        }
        
        return summary
    
    def get_prediction_with_explanation(self, X_new: Union[pd.DataFrame, Dict],
                                       model_type: str = 'RandomForest',
                                       top_features: int = 5) -> Dict:
        """
        ì˜ˆì¸¡ ê²°ê³¼ì™€ ì„¤ëª… ì •ë³´ ë°˜í™˜ (Streamlit í‘œì‹œìš©)
        
        Args:
            X_new: ìƒˆë¡œìš´ í™˜ì ë°ì´í„°
            model_type: ì‚¬ìš©í•  ëª¨ë¸
            top_features: í‘œì‹œí•  ì£¼ìš” íŠ¹ì„± ê°œìˆ˜
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë° ì„¤ëª… ì •ë³´
        """
        # ê¸°ë³¸ ì˜ˆì¸¡
        pred_result = self.predict(X_new, model_type)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ê°€
        try:
            importance_df = self.get_feature_importance(model_type, top_n=top_features)
            pred_result['top_features'] = importance_df.head(top_features).to_dict('records')
        except:
            pred_result['top_features'] = None
        
        # ì…ë ¥ ë°ì´í„°ì˜ ì£¼ìš” ê°’ ì¶”ê°€
        if isinstance(X_new, dict):
            X_df = pd.DataFrame([X_new])
        else:
            X_df = X_new
        
        pred_result['input_summary'] = {
            'n_features': len(X_df.columns),
            'sample_values': X_df.iloc[0].to_dict() if len(X_df) > 0 else {}
        }
        
        return pred_result
    
    def validate_input_data(self, X_new: Union[pd.DataFrame, Dict]) -> Dict:
        """
        ì…ë ¥ ë°ì´í„° ê²€ì¦ (Streamlitì—ì„œ ì‚¬ìš©)
        
        Args:
            X_new: ê²€ì¦í•  ë°ì´í„°
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            'valid': True,
            'errors': [],
            'warnings': []
        }
        
        # DataFrame ë³€í™˜
        if isinstance(X_new, dict):
            X_df = pd.DataFrame([X_new])
        elif isinstance(X_new, pd.Series):
            X_df = pd.DataFrame([X_new])
        else:
            X_df = X_new.copy()
        
        # íŠ¹ì„± ì´ë¦„ í™•ì¸
        if not self.feature_names:
            result['valid'] = False
            result['errors'].append("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. feature_namesê°€ ì—†ìŠµë‹ˆë‹¤.")
            return result
        
        # ëˆ„ë½ëœ íŠ¹ì„± í™•ì¸
        missing_features = set(self.feature_names) - set(X_df.columns)
        if missing_features:
            result['valid'] = False
            result['errors'].append(f"ëˆ„ë½ëœ íŠ¹ì„±: {list(missing_features)[:10]}")
        
        # ì¶”ê°€ íŠ¹ì„± í™•ì¸ (ê²½ê³ )
        extra_features = set(X_df.columns) - set(self.feature_names)
        if extra_features:
            result['warnings'].append(f"ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì´ ìˆìŠµë‹ˆë‹¤ (ë¬´ì‹œë¨): {list(extra_features)[:10]}")
        
        # ê²°ì¸¡ê°’ í™•ì¸
        if X_df.isnull().any().any():
            null_cols = X_df.columns[X_df.isnull().any()].tolist()
            result['warnings'].append(f"ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼: {null_cols[:10]}")
        
        # ë¬´í•œëŒ€ ê°’ í™•ì¸
        numeric_cols = X_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if np.isinf(X_df[col]).any():
                result['warnings'].append(f"ë¬´í•œëŒ€ ê°’ì´ ìˆëŠ” ì»¬ëŸ¼: {col}")
        
        return result
    
    def get_sample_data(self, n_samples: int = 5) -> pd.DataFrame:
        """
        í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜ (Streamlit ë°ëª¨ìš©)
        
        Args:
            n_samples: ë°˜í™˜í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            ìƒ˜í”Œ ë°ì´í„° DataFrame
        """
        if 'X_test' not in self.models:
            raise ValueError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        X_test = self.models['X_test']
        return X_test.head(n_samples)
    
    def plot_prediction_results(self, model_type: str = 'RandomForest',
                               save_path: Optional[str] = None):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” (Streamlitìš©)
        
        Args:
            model_type: ì‹œê°í™”í•  ëª¨ë¸
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í‘œì‹œë§Œ)
        
        Returns:
            matplotlib figure ê°ì²´
        """
        if model_type not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_type}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        y_test = self.models['y_test']
        y_pred = self.models[model_type]['test_predictions']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’
        axes[0, 0].scatter(y_test, y_pred, alpha=0.5, s=50)
        axes[0, 0].plot([y_test.min(), y_test.max()], 
                        [y_test.min(), y_test.max()], 
                        'r--', lw=2, label='Perfect')
        axes[0, 0].set_xlabel('Actual BP (mmHg)', fontsize=12)
        axes[0, 0].set_ylabel('Predicted BP (mmHg)', fontsize=12)
        axes[0, 0].set_title(f'Actual vs Predicted - {model_type}', 
                            fontsize=14, fontweight='bold')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ì”ì°¨ í”Œë¡¯
        residuals = y_test - y_pred
        axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=50)
        axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)
        axes[0, 1].set_xlabel('Predicted BP (mmHg)', fontsize=12)
        axes[0, 1].set_ylabel('Residuals (mmHg)', fontsize=12)
        axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ì”ì°¨ ë¶„í¬
        axes[1, 0].hist(residuals, bins=30, edgecolor='black', 
                       alpha=0.7, color='skyblue')
        axes[1, 0].axvline(x=0, color='r', linestyle='--', lw=2)
        axes[1, 0].set_xlabel('Residuals (mmHg)', fontsize=12)
        axes[1, 0].set_ylabel('Frequency', fontsize=12)
        axes[1, 0].set_title('Residual Distribution', 
                            fontsize=14, fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # 4. ì„±ëŠ¥ ì§€í‘œ
        mae = self.models[model_type]['test_mae']
        rmse = self.models[model_type]['test_rmse']
        r2 = self.models[model_type]['test_r2']
        
        metrics_text = f"Performance Metrics\n\n"
        metrics_text += f"MAE:  {mae:.2f} mmHg\n"
        metrics_text += f"RMSE: {rmse:.2f} mmHg\n"
        metrics_text += f"RÂ²:   {r2:.3f}\n\n"
        metrics_text += f"Residual Stats:\n"
        metrics_text += f"Mean: {residuals.mean():.2f}\n"
        metrics_text += f"Std:  {residuals.std():.2f}"
        
        axes[1, 1].text(0.5, 0.5, metrics_text, 
                       ha='center', va='center',
                       fontsize=14, family='monospace',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {save_path}")
        
        return fig
    
    def plot_feature_importance(self, model_type: str = 'RandomForest',
                               top_n: int = 15,
                               save_path: Optional[str] = None):
        """
        íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” (Streamlitìš©)
        
        Args:
            model_type: ì‹œê°í™”í•  ëª¨ë¸
            top_n: í‘œì‹œí•  ìƒìœ„ íŠ¹ì„± ê°œìˆ˜
            save_path: ì €ì¥ ê²½ë¡œ
        
        Returns:
            matplotlib figure ê°ì²´
        """
        importance_df = self.get_feature_importance(model_type, top_n=len(self.feature_names))
        top_features = importance_df.head(top_n)
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. ìƒìœ„ íŠ¹ì„± ë°” ì°¨íŠ¸
        axes[0].barh(range(len(top_features)), top_features['importance'], 
                    color='steelblue', edgecolor='black', alpha=0.7)
        axes[0].set_yticks(range(len(top_features)))
        axes[0].set_yticklabels(top_features['feature'])
        axes[0].set_xlabel('Importance', fontsize=12)
        axes[0].set_title(f'Top {top_n} Features - {model_type}', 
                         fontsize=14, fontweight='bold')
        axes[0].grid(True, alpha=0.3, axis='x')
        axes[0].invert_yaxis()
        
        # 2. ëˆ„ì  ì¤‘ìš”ë„
        cumsum = importance_df['importance'].cumsum()
        cumsum_pct = (cumsum / cumsum.iloc[-1]) * 100
        
        axes[1].plot(range(1, len(cumsum_pct) + 1), cumsum_pct, 
                    marker='o', linewidth=2, markersize=4)
        axes[1].axhline(y=80, color='r', linestyle='--', 
                       linewidth=2, label='80%')
        axes[1].axhline(y=90, color='orange', linestyle='--', 
                       linewidth=2, label='90%')
        axes[1].set_xlabel('Number of Features', fontsize=12)
        axes[1].set_ylabel('Cumulative Importance (%)', fontsize=12)
        axes[1].set_title('Cumulative Importance', 
                         fontsize=14, fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {save_path}")
        
        return fig

    # ================================================================
    # í†µí•© íŒŒì´í”„ë¼ì¸ ë©”ì„œë“œ
    # ================================================================
    
    def save_model(self, model_type: str, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        if model_type not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_type}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        model = self.models[model_type]['model']
        joblib.dump(model, filepath)
        print(f"âœ… ëª¨ë¸ ì €ì¥: {filepath}")
    
    def save_scaler(self, filepath: str):
        """ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
        if self.scaler is None:
            raise ValueError("ì €ì¥í•  ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        joblib.dump(self.scaler, filepath)
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {filepath}")
    
    def save_metadata(self, filepath: str, target_column: str = 'NIBP_mean'):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        best_name = self.models.get('best_model_name')
        
        if best_name is None:
            raise ValueError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        metadata = {
            'model_name': best_name,
            'test_mae': float(self.models[best_name]['test_mae']),
            'test_rmse': float(self.models[best_name]['test_rmse']),
            'test_r2': float(self.models[best_name]['test_r2']),
            'n_features': len(self.feature_names),
            'features': self.feature_names,
            'target_column': target_column,
            'train_date': str(pd.Timestamp.now())
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {filepath}")
    
    def load_model(self, model_type: str, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        model = joblib.load(filepath)
        if model_type not in self.models:
            self.models[model_type] = {}
        self.models[model_type]['model'] = model
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {filepath}")
    
    def load_scaler(self, filepath: str):
        """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        self.scaler = joblib.load(filepath)
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {filepath}")
    
    # ================================================================
    # í†µí•© íŒŒì´í”„ë¼ì¸ ë©”ì„œë“œ
    # ================================================================
    
    def full_pipeline(self, max_records: Optional[int] = 100,
                     target_col: str = 'NIBP_mean',
                     test_size: float = 0.2) -> Dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            max_records: ë¡œë“œí•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜
            target_col: íƒ€ê²Ÿ ë³€ìˆ˜
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        print("=" * 80)
        print("ğŸ©º PhysioNet í˜ˆì•• ì˜ˆì¸¡ AI ì‹œìŠ¤í…œ")
        print("=" * 80)
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_all_patient_data(max_records=max_records)
        
        # 2. ì „ì²˜ë¦¬
        df_clean = self.preprocess_data(df, target_column=target_col)
        
        # 3. íŠ¹ì„± ì¤€ë¹„
        X, y, features = self.prepare_features(df_clean, target_col=target_col)
        
        # 4. ëª¨ë¸ í•™ìŠµ
        results = self.train_models(X, y, test_size=test_size)
        
        print("\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        
        return results



